# pip install google-generativeai mcp
import asyncio
from datetime import timedelta
# Add json import for formatting output
import json
from google import genai
from google.genai import types
from mcp import ClientSession, ListPromptsResult
from mcp.client.streamable_http  import streamablehttp_client 
from dotenv import dotenv_values

config = dotenv_values(".env")

client = genai.Client(api_key=config.get("GEMINI_API_KEY"))



async def get_ia_answers(prompt:str,tools:list[types.Tool],session:ClientSession, mcp_prompts:ListPromptsResult):
    response = client.models.generate_content(
        model="gemini-2.5-flash",
        contents=prompt,
        config=types.GenerateContentConfig(
            temperature=0,
            tools=tools,
            #system_instruction="You are a helpful assistant that can answer questions about public transport in the Paris region. You can use the MCP server to get information about transport lines and messages.",
        ),
    )
    

    # Remove raw response print
    if response and response.candidates and response.candidates[0].content and response.candidates[0].content.parts and response.candidates[0].content.parts[0].function_call:
        function_call = response.candidates[0].content.parts[0].function_call

        if not function_call.name:
            print("Model did not generate a function call.")
            return

        result = await session.call_tool(
            function_call.name, arguments=dict(function_call.args)
        )

        # Parse and print formatted JSON result
        print("--- Formatted Result ---") # Add header for clarity
        try:
            if not result.content:
                print("No content returned from MCP server.")
                return

            mcp_data = json.loads(result.content[0].text)

            possiblePrompt = [prompt for prompt in mcp_prompts.prompts if prompt.name == function_call.name + "_prompt"]
            if possiblePrompt:
                formatedPrompt =await session.get_prompt(
                    possiblePrompt[0].name, arguments={key.name:json.dumps(mcp_data) for key in possiblePrompt[0].arguments}
                )

                responseFormated = client.models.generate_content(
                    model="gemini-2.5-flash",
                    contents=formatedPrompt.messages[0].content,
                    config=types.GenerateContentConfig(
                        temperature=0,
                        tools=[],
                    ),
                )
                print(responseFormated.text)

            else:
                print(json.dumps(mcp_data, indent=2))


        except json.JSONDecodeError:
            print("MCP server returned non-JSON response:")
            print(result.content[0].text)
        except (IndexError, AttributeError):
                print("Unexpected result structure from MCP server:")
                print(result)
    else:
        print("No function call was generated by the model.")
        if response.text:
            print("Model response:")
            print(response.text)

async def run():
    # Remove debug prints
    async with streamablehttp_client(timeout=timedelta(seconds=60),url="http://localhost:8000/mcp") as (read_stream, write_stream,get_session_id):
        async with ClientSession(read_stream, write_stream) as session:
            await session.initialize()

            if get_session_id:
                session_id = get_session_id()
                if session_id:
                    print(f"Session ID: {session_id}")
            mcp_tools = await session.list_tools()

            mcp_prompts = await session.list_prompts()
            #mcp_resources = await session.list_resources()

            tools = [
                types.Tool(
                    function_declarations=[
                        {
                            "name": tool.name,
                            "description": tool.description,
                            "parameters": {
                                k: v
                                for k, v in tool.inputSchema.items()
                                if k not in ["additionalProperties", "$schema"]
                            },
                        }
                    ]
                )
                for tool in mcp_tools.tools
            ]

            # gestion du prompt

            print('Enter a prompt for the AI to process.')
            print('To exit, enter `exit`.')
            while True:
                raw_input = input('> ')
                print(raw_input)
                if raw_input=='exit':
                    break
                else:
                    await get_ia_answers(raw_input, tools, session, mcp_prompts)


# Revert main block
if __name__ == "__main__":
    # Use asyncio.run to execute the run function
    print("Starting MCP clientâ€¦")

    
    asyncio.run(run())